# CSE 446: Machine Learning


### Lecture Topics

##### Lecture 1: Maximum Likelihood Estimation
- Maximum Likelihood estimation

##### Lecture 2: Linear Regression
- Least Square errors
- Justification for minimizing squared error ( Error ~ N(0,1))

##### Lecture 3: Linear Regression 2
- Wrap-up of Least Square Errors
- Gradient Descent


##### Lecture 4: Regression wrap-up AND Bias-Variance Trade-off
- Assessing performance of regression model --> determining loss/cost
- Overfitting
- Generalization (true) Error
- error = Bias, Variance, Noise
- Bias-Variance Trade-off in model complexity

##### Lecture 5: Bias Variance Tradeoff 2 & Ridge Regression
- Regularization: dealing with infinitely many solutions
- Ridge regression --> adding curvature 
- Bias, variance, and irreducible error

##### Lecture 6: Cross validation
- Importance of validation
- Leave one out validation
- K-fold validation
- Choosing hyperparameters


##### Lecture 6: Cross-validation
- Choosing hyperparameters (i.e. lambda for ridge regression/lasso)
- Leave One Out validation
- K-fold cross validation

##### Lecture 7: LASSO
- Benefits of L1 Regularization

### Homework Topics

##### HW0 : Review
- Probability review
- Expectation, variance
- Linear algebra review
- intro to python

##### HW1
- Maximum Likelihood Estimation (MLE)
- Bias - Variance trade-off
- Linear Regression
- Ridge Regression
- Test error and training error
