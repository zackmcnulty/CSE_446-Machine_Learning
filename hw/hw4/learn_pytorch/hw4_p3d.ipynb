{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# LOADS CIFAR10 images which are 32 x 32 x 3 RGB images\n",
    "# iter(trainloader/testloader) are iterables that come in pairs (image, label)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12 # number of epochs to train for\n",
    "momentum = 0.9 # momentum for Stochastic Gradient Descent\n",
    "lr = 0.001 # learning rate (eta) for gradient descent\n",
    "M = 100  # number of neurons in hidden layer of neural network\n",
    "p = 5  # filter window size \n",
    "N = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, M, N, p):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # The 3 input channels are the RGB of the image\n",
    "        # the Kernel size is the size of the filter window (p x p in this case)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=M, kernel_size=p, bias=True, padding=p)\n",
    "        self.conv2 = nn.Conv2d(in_channels=M, out_channels=M, kernel_size=p, bias=True, padding=p)\n",
    "        self.conv3 = nn.Conv2d(in_channels=M, out_channels=M, kernel_size=p, bias=True, padding=p)\n",
    "        \n",
    "        # Max pooling layer. kernel_size is the size of the window that is used. Max is selected within a N x N window\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.linear = nn.Linear(((33) // 8)**2 * M,  10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # vectorize the data before feeding it into the linear layer. Note the 4 is due to the batch of size 4 used\n",
    "        # (see homework specification)\n",
    "        x = x.view(4, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "net = Net(M, N, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define a Loss function and optimizer\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the network\n",
    "\n",
    "\n",
    "This is when things start to get interesting.\n",
    "We simply have to loop over our data iterator, and feed the inputs to the\n",
    "network and optimize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.952\n",
      "[1,  4000] loss: 1.706\n",
      "[1,  6000] loss: 1.566\n",
      "[1,  8000] loss: 1.490\n",
      "[1, 10000] loss: 1.450\n",
      "[1, 12000] loss: 1.408\n",
      "END OF EPOCH  1 : train accuracy =  52.632  // test accuracy =  51.95\n",
      "[2,  2000] loss: 1.337\n",
      "[2,  4000] loss: 1.324\n",
      "[2,  6000] loss: 1.292\n",
      "[2,  8000] loss: 1.292\n",
      "[2, 10000] loss: 1.281\n",
      "[2, 12000] loss: 1.237\n",
      "END OF EPOCH  2 : train accuracy =  58.058  // test accuracy =  56.74\n",
      "[3,  2000] loss: 1.219\n",
      "[3,  4000] loss: 1.212\n",
      "[3,  6000] loss: 1.204\n",
      "[3,  8000] loss: 1.235\n",
      "[3, 10000] loss: 1.185\n",
      "[3, 12000] loss: 1.198\n",
      "END OF EPOCH  3 : train accuracy =  61.292  // test accuracy =  59.87\n",
      "[4,  2000] loss: 1.166\n",
      "[4,  4000] loss: 1.187\n",
      "[4,  6000] loss: 1.165\n",
      "[4,  8000] loss: 1.136\n",
      "[4, 10000] loss: 1.154\n",
      "[4, 12000] loss: 1.150\n",
      "END OF EPOCH  4 : train accuracy =  61.486  // test accuracy =  59.0\n"
     ]
    }
   ],
   "source": [
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    train_accuracy = calc_accuracy(trainloader)\n",
    "    test_accuracy = calc_accuracy(testloader)\n",
    "    print('END OF EPOCH ', epoch + 1, ': train accuracy = ', train_accuracy, ' // test accuracy = ', test_accuracy)\n",
    "    \n",
    "    all_train_accuracies.append(train_accuracy)\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot accuracy over time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(all_train_accuracies)\n",
    "plt.plot(all_test_accuracies)\n",
    "plt.title('Training and Testing Accuracy after each iteration')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(['Training', 'Testing'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
